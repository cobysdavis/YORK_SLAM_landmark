#!/usr/bin/env python
import cv2
from matplotlib import pyplot as plt
from random import randint, sample
import math
from tempfile import TemporaryFile
import numpy as np

def translate(value, leftMin, leftMax, rightMin, rightMax):
    # Figure out how 'wide' each range is
    leftSpan = leftMax - leftMin
    rightSpan = rightMax - rightMin

    # Convert the left range into a 0-1 range (float)
    valueScaled = float(value - leftMin) / float(leftSpan)

    # Convert the 0-1 range into a value in the right range.
    return rightMin + (valueScaled * rightSpan)

def distance(v1, v2):
    return np.sqrt(np.sum((np.array(v1) - np.array(v2)) ** 2))   

def inumerate(objs, indices):
	for idx in indices:
		yield objs[idx], idx

def outputMatches(matches, kps, thresh):
	chosen_matches = []
	chosen_keypoints = []
	# keeps shuffle and removing indices to iterate thru
	indices = range(len(matches))
	while len(chosen_matches) < 4:
		for match, index in inumerate(matches, sample(indices, len(indices))):
			if len(chosen_matches) >= 4:
				break
			if not close_to_other_keypoints(chosen_keypoints, kps[match.queryIdx], thresh):
				chosen_matches.append(match)
				chosen_keypoints.append(kps[match.queryIdx])
				indices.remove(index)
			pass
		pass	
	return chosen_matches
		

def output4Matches(good_matches,kp):
	list=[]
	prev=[]
	thresh=100
	print len(good_matches)
	print len(kp)
	while len(list)<4:
		x=randint(0,len(good_matches)-1)
		if not already_chosen(prev,x):
			if not close_to_others(prev,x,thresh,kp):
				list.append(good_matches[x])
				prev.append(x)
				print "good match"
	for l in prev:
		print "point:" + str(kp[l].pt)
	for l in list:
		print kp[l.trainIdx].pt
	for l in list:
		print kp[l.trainIdx].pt
	return list

def close_to_other_keypoints(kps, keypoint, thresh):
	if len(kps) == 0:
		return False
	for kp in kps:
		if distance(kp.pt, keypoint.pt) < thresh:
			return True
	return False

def keypoint_lists(points, kp1, kp2):
	a = np.float32([kp1[p.queryIdx].pt for p in points])
	b = np.float32([kp2[p.trainIdx].pt for p in points])
	return a, b

def create_same_points(points,kp1,kp2):
	list=[]
	list.append([])
	list.append([])
	for p in points:
		list[0].append(kp1[p.queryIdx].pt)
		list[1].append(kp2[p.trainIdx].pt)
	a=np.float32(list[0])
	b=np.float32(list[1])
	list2=[]
	list2.append(a)
	list2.append(b)
	return list2
	

def drawMatches(img1, img2,list1,list2):
    # Create a new output image that concatenates the two images together
    # (a.k.a) a montage
    rows1 = img1.shape[0]
    cols1 = img1.shape[1]
    rows2 = img2.shape[0]
    cols2 = img2.shape[1]

    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')

    # Place the first image to the left
    out[:rows1,:cols1] = np.dstack([img1, img1, img1])

    # Place the next image to the right of it
    out[:rows2,cols1:] = np.dstack([img2, img2, img2])

    # For each pair of points we have between both images
    # draw circles, then connect a line between them
    for i in range(0,len(list1)):
	
        # x - columns
        # y - rows
        (x1,y1) = list1[i]
        (x2,y2) = list2[i]
	#print str((x1,y1))
	#print str((x2,y2))
        # Draw a small circle at both co-ordinates
        # radius 4
        # colour blue
        # thickness = 1
        cv2.circle(out, (int(x1),int(y1)), 8, (255, 0, 0), 7)   
        cv2.circle(out, (int(x2)+cols1,int(y2)), 8, (255, 0, 0), 7)

        # Draw a line in between the two points
        # thickness = 1
        # colour blue
        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (255, 0, 0), 8)


    # Show the image
    #cv2.imshow('Matched Features', out)
    cv2.waitKey(0)
    cv2.destroyWindow('Matched Features')

    # Also return the image if you'd like a copy
    return out


img1 = cv2.imread('/home/vgrlab/catkin_ws/src/YORK_SLAM_landmark/mcptam/scripts/1.jpg',0) # queryImage
img2 = cv2.imread('/home/vgrlab/catkin_ws/src/YORK_SLAM_landmark/mcptam/scripts/logo_colour.jpg',0) # trainImage
threshhold=0.3

# Initiate SIFT detector
sift = cv2.xfeatures2d.SIFT_create()


# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)


# BFMatcher with default params
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1,des2, k=2)

#applies for cv2.perspectiveTransform previous version before using homography

# Apply ratio test
good = []
for m,n in matches:
    if m.distance < threshhold*n.distance:
	good.append([m])
good2=[]
for m,n in matches:
    if m.distance < threshhold*n.distance:
	good2.append(m)


# cv2.drawMatchesKnn expects list of lists as matches.
img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)
plt.imshow(img3),plt.show()

#output 4 necessary matches
#matching_points=outputMatches(good2, kp1, 150)
matching_points = good2
#print matching_points

query_list, train_list = keypoint_lists(matching_points, kp1, kp2)

#pre Robert
#point_list=create_same_points(matching_points,kp1,kp2)
#query_points, train_point = point_list[0], point_list[1]

#draw 4  matching points
img4 = drawMatches(img1,img2,query_list,train_list)
plt.imshow(img4),plt.show()

query_height, query_width = img1.shape[:2]
train_height, train_width = img2.shape[:2]

#create transfomration matrix
homography, mask = cv2.findHomography(query_list, train_list, cv2.RANSAC)
print homography
#pre Robert
#M = cv2.getPerspectiveTransform(train_list,query_list)

ftr_corners = np.float32([[0, 0], [train_width, 0], [train_width, train_height], [0, train_width]]).reshape(1, -1, 2)
corners = np.int32( cv2.perspectiveTransform(ftr_corners, homography).reshape(-1, 2) )

#pre Robert
#print M
#matrix=[]
#for l in M:
#	for p in l:
#		matrix.append(float(p))
#print matrix
#outfile = TemporaryFile()
#np.save(outfile, matrix)
#np.savetxt('/home/vgrlab/catkin_ws/src/mcptam/scripts/matrix.txt', matrix)

#apply to query Image to see if matrix is correct
dst1 = cv2.warpPerspective(img1,homography, (train_width, train_height))
plt.subplot(121),plt.imshow(img1),plt.title('Input')
plt.subplot(122),plt.imshow(dst1),plt.title('Output')
plt.show()

#!/usr/bin/env python
 
import cv2
import numpy as np
 
# Read Image
im = cv2.imread('/home/vgrlab/catkin_ws/src/YORK_SLAM_landmark/mcptam/scripts/1.jpg');
size = im.shape

#2D image points. If you change the image, you need to change vector
'''
image_points = np.array([
                            (359, 391),     # Nose tip
                            (399, 561),     # Chin
                            (337, 297),     # Left eye left corner
                            (513, 301),     # Right eye right corne
                            (345, 465),     # Left Mouth corner
                            (453, 469)      # Right mouth corner
                        ], dtype="double")
print image_points
'''
temp=[]
for m in matching_points:
	temp.append(kp1[m.queryIdx].pt)
image_points=np.array(temp)
print "image points: "
print image_points

#training image is 8.5x11 in^2=0.2159 x 0.2794 m^2
#define (0,0,0)= top left of training image
'''
# 3D model points.
model_points = np.array([
                            (0.0, 0.0, 0.0),             # Nose tip
                            (0.0, -330.0, -65.0),        # Chin
                            (-225.0, 170.0, -135.0),     # Left eye left corner
                            (225.0, 170.0, -135.0),      # Right eye right corne
                            (-150.0, -150.0, -125.0),    # Left Mouth corner
                            (150.0, -150.0, -125.0)      # Right mouth corner
                         
                        ])
 '''

train_width_in_m=0.2794
train_height_in_m=0.2159

temp=[]
for m in matching_points:
	real_x=translate(kp1[m.trainIdx].pt[0], 0, query_width, 0, train_width_in_m)
	real_y=translate(kp1[m.trainIdx].pt[1], 0, -query_height, 0, train_height_in_m)
	temp.append((real_x,real_y,0))
model_points=np.array(temp)
print "model points: "
print model_points

# Camera internals
 
focal_length = size[1]
center = (size[1]/2, size[0]/2)
camera_matrix = np.array(
                         [[focal_length, 0, center[0]],
                         [0, focal_length, center[1]],
                         [0, 0, 1]], dtype = "double"
                         )
 
print "Camera Matrix :\n {0}".format(camera_matrix)
 
dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion
(success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=SOLVEPNP_ITERATIVE)
 
print "Rotation Vector:\n {0}".format(rotation_vector)
print "Translation Vector:\n {0}".format(translation_vector)

 
# Project a 3D point (0, 0, 1000.0) onto the image plane.
# We use this to draw a line sticking out of the nose
 
 
(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)
 
for p in image_points:
    cv2.circle(im, (int(p[0]), int(p[1])), 3, (0,0,255), -1)
 
 
p1 = ( int(image_points[0][0]), int(image_points[0][1]))
p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))
 
cv2.line(im, p1, p2, (255,0,0), 2)
 
# Display image
cv2.imshow("Output", im)
cv2.waitKey(0)


